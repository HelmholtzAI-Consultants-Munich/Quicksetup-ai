original_work_dir: ${hydra:runtime.cwd}
data_dir: ${original_work_dir}/data/
print_config: true
ignore_warnings: true
train: true
test: true
seed: null
name: default
datamodule:
  _target_: ml_pipeline_template.datamodules.mnist_datamodule.MNISTDataModule
  data_dir: ${data_dir}
  batch_size: 64
  train_val_test_split:
  - 55000
  - 5000
  - 10000
  num_workers: 0
  pin_memory: false
model:
  _target_: ml_pipeline_template.models.mnist_module.MNISTLitModule
  lr: 0.001
  weight_decay: 0.0005
  net:
    _target_: ml_pipeline_template.models.components.simple_dense_net.SimpleDenseNet
    input_size: 784
    lin1_size: 256
    lin2_size: 256
    lin3_size: 256
    output_size: 10
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/acc
    mode: max
    save_top_k: 1
    save_last: true
    verbose: false
    dirpath: checkpoints/
    filename: epoch_{epoch:03d}
    auto_insert_metric_name: false
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/acc
    mode: max
    patience: 100
    min_delta: 0
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
logger:
  mlflow:
    _target_: pytorch_lightning.loggers.mlflow.MLFlowLogger
    experiment_name: ${name}
    tracking_uri: null
    tags: null
    save_dir: ${original_work_dir}/mlruns
    prefix: ''
    artifact_location: null
trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 1
  max_epochs: 10
  resume_from_checkpoint: null
